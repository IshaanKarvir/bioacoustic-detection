{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHaN-0hcHP_y"
   },
   "source": [
    "# Instructions to Use this Notebook\n",
    "\n",
    "1. To run this notebook, first run `Libraries to Install`, `Imports`, and the subsections of`Constants and Classes` (`Constants`, `Model`, and `Annotation_Maker`)in that order. Then depending on if you want to use `Generate CSV` or `Generate Training Data`, run that cell specifically. Make sure you have modified whatever user parameters you would like to use before running this cell.\n",
    "\n",
    "2. User adjustable parameters are located towards the bottom of this notebook under `Generate CSV` and `Generate Training Data`. What they do will be explained there.\n",
    " \n",
    "3. At the very bottom of this file are the times of how long it took to run different set ups to give you an idea of your runtime (e.g spectral subtaction vs no spectral subtraction, 11 minute file vs 3 hour long file, generating images with predictions drawn on top versus without)\n",
    "\n",
    "### Common Troubleshooting Issues For When the Code Does Not Run\n",
    "- Make sure the paths are valid and formatted properly\n",
    "  - If the file is a directory it must have a / at the end\n",
    "  - If the file is an audio file or model path file, make sure it exists and that the path is correct\n",
    "- If the code crashes due to memory issues, try splitting the audio file into multiple, shorter audio files. You can monitor memory in the right corner of this notebook in colab\n",
    "- If you didn't hit run all to run this notebook, make sure you run the cells in order starting from `Libraries to Install`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro2dpnLYCrRm"
   },
   "source": [
    "### Libraries to Install\n",
    "\n",
    "Run the following cell to install libraries not already installed onto Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvF-o4-Rvw6o",
    "outputId": "05d61782-1f9c-4176-cb2d-881b7438474a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eaYoiwYDAYs"
   },
   "source": [
    "### Imports\n",
    "\n",
    "The following imports are used to create the `Annotation_Maker` and `Model` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpEncPHCh6u7"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image,ImageDraw\n",
    "import os\n",
    "\n",
    "import glob\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import csv\n",
    "import gc \n",
    "\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings( \"ignore\") #surpresses a divide by 0 warning that's not really an issuesince the library handles it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4nZCRmXUrlc"
   },
   "source": [
    "### Constants and Classes\n",
    "\n",
    "There are two classes defined in the following cell: `Model` and `Annotation_Maker`. `Model` is used as a wrapper class for the the `tflite` model generated by Google Cloud Vision. `Annotation_Maker` uses `Model` to generate a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y87AU5sqqJu4"
   },
   "source": [
    "#### Constants\n",
    "\n",
    "Important variables used in the classes `Model` and `Annotation_Maker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3ZKNbpwqVcO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# time constants for pydub\n",
    "minute = 1000*60\n",
    "second = 1000\n",
    "half_minute = 1000*30\n",
    "\n",
    "# pyplot spectrogram constants\n",
    "ymax = 2000 #max frequency\n",
    "dim = 11.05 #image dimensions (forces the image to be a square image whose sides are 600px )\n",
    "prefix = 'whale_'\n",
    "\n",
    "# converting to time_frequency dictionary constants\n",
    "frequency_max = 2000\n",
    "time_max = 60\n",
    "\n",
    "# image scaling constants\n",
    "W_H = 600 #pixel dimensions of image\n",
    "\n",
    "# spectral subtraction constant to convert pydub segments to work with librosa\n",
    "int16_max = 32767\n",
    "\n",
    "# class mappings\n",
    "class_dict = {\n",
    "    0: \"hb whale\",\n",
    "    1: \"other\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9jTykM-qbIh"
   },
   "source": [
    "#### Model\n",
    "\n",
    "The wrapper class for the `tflite` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8Lhg35WEmxn"
   },
   "outputs": [],
   "source": [
    "\n",
    "# wrapper class for the tflite model, used in the Annotation_Maker class\n",
    "class Model:\n",
    "  \n",
    "  def __init__(self, tf_path, thresh = 0.5):\n",
    "    # thresh is the thresh hold of confidence for a prediction\n",
    "    self.interpreter = None\n",
    "    self.input_details = None\n",
    "    self.input_shape = None\n",
    "\n",
    "    self._init_model(tf_path)\n",
    "    self.thresh = thresh\n",
    "    self.d_x = self.input_shape[1]\n",
    "    self.d_y = self.input_shape[2]\n",
    "\n",
    "  def _init_model(self, tf_path):\n",
    "    #helper function to initialize the variables in init\n",
    "    self.interpreter = tf.lite.Interpreter(model_path=tf_path)\n",
    "    self.input_details = self.interpreter.get_input_details()[0]\n",
    "    self.input_shape = self.input_details['shape']\n",
    "    self.output_details = self.interpreter.get_output_details()\n",
    "    self.interpreter.allocate_tensors()  \n",
    "\n",
    "  \n",
    "  def _reshape_input(self, image_array):\n",
    "    # converts the image into a numpy array and rearranges it to what the tflite model expects\n",
    "    return [np.array(Image.open(img).resize((self.d_x, self.d_y)))[:, :,:3].reshape(1,self.d_x, self.d_y,3) for img in image_array]\n",
    "\n",
    "\n",
    "  \n",
    "  def _filter_preds(self, image_name):\n",
    "      # formats the predictions from the tflite model in a usable way\n",
    "      # also filters out predictions whose scores are below a certain thresh hold\n",
    "\n",
    "      num_det = int(self.interpreter.get_tensor(self.output_details[3]['index'])[0])\n",
    "      boxes = self.interpreter.get_tensor(self.output_details[0]['index'])[0]\n",
    "      classes = self.interpreter.get_tensor(self.output_details[1]['index'])[0]\n",
    "      scores = self.interpreter.get_tensor(self.output_details[2]['index'])[0]\n",
    "  \n",
    "      ret_boxes = []\n",
    "      ret_classes = []\n",
    "      ret_scores = []\n",
    "      ret_det = num_det              \n",
    "                                     \n",
    "      for i in range(num_det):\n",
    "\n",
    "        if scores[i] >= self.thresh:\n",
    "          ret_boxes.append(boxes[i])\n",
    "          ret_classes.append(classes[i])\n",
    "          ret_scores.append(scores[i])\n",
    "        else:\n",
    "          ret_det -= 1\n",
    "\n",
    "      return {\n",
    "          \"image_name\": image_name,\n",
    "          \"boxes\": ret_boxes,\n",
    "          \"classes\": ret_classes,\n",
    "          \"scores\": ret_scores,\n",
    "          \"num_det\":ret_det\n",
    "      }\n",
    "\n",
    "\n",
    "  def _draw_box(self, pred, dir):\n",
    "    # draws prediction boxes onto a single image (classes are not labeled though)\n",
    "     \n",
    "    img = Image.open(pred[\"image_name\"])\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    for box in pred[\"boxes\"]:\n",
    "      new_box = box * W_H\n",
    "      x0 = new_box[1]\n",
    "      x1 = new_box[3]\n",
    "\n",
    "      y0 = new_box[0]\n",
    "      y1 = new_box[2]\n",
    "\n",
    "      draw.rectangle([x0, y0, x1,y1 ],outline = \"black\")\n",
    "\n",
    "    img.save(dir + os.path.basename(pred[\"image_name\"]))\n",
    "\n",
    "\n",
    "  def get_details(self):\n",
    "    # get the tensor details about the tf_lite model\n",
    "    return {\n",
    "        \"input\": self.interpreter.get_input_details(),\n",
    "        \"output\": self.interpreter.get_output_details()\n",
    "    }\n",
    "      \n",
    "\n",
    "  def predict(self, image_array):\n",
    "    # returns the predictions for each image\n",
    "    input_data = self._reshape_input(image_array)\n",
    "    image_preds = []\n",
    "\n",
    "    for i in range(len(image_array)):\n",
    "      self.interpreter.set_tensor(self.input_details['index'], input_data[i])\n",
    "      self.interpreter.invoke()\n",
    "      image_preds.append(self._filter_preds(image_array[i]))\n",
    " \n",
    "    return image_preds\n",
    "\n",
    "    \n",
    "  def export_boxed_images(self, prediction_dict, dir_path):\n",
    "    # draws the predictions for all images that are predicted on (classes are not labeled though)\n",
    "    for pred in prediction_dict:\n",
    "      self._draw_box(pred, dir_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9YzFaBiqo5U"
   },
   "source": [
    "#### Annotation_Maker\n",
    "\n",
    "The class that generates both the csv and training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqCk5aHSqm1n"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Annotation_Maker:\n",
    "\n",
    "  def __init__(self, tf_path = None, spectral_subtraction_noise_file = None, thresh_model = 0.5, thresh_area = 0.5):\n",
    "    \n",
    "    # if tf_path is not defined, you can only generate training images\n",
    "    if tf_path:\n",
    "      self.model = Model(tf_path, thresh_model)\n",
    "\n",
    "    # self.mns will indicate if we are using spectral subtraction\n",
    "    if spectral_subtraction_noise_file:\n",
    "      nw, nsr = librosa.load(spectral_subtraction_noise_file, sr=None, mono=True)\n",
    "      self.mns= np.mean(np.abs(librosa.stft(nw)), axis=1)\n",
    "      del nw, nsr\n",
    "      self.mns = self.mns.reshape((self.mns.shape[0],1))\n",
    "    else:\n",
    "      self.mns = None\n",
    "\n",
    "    # thresh hold for overlapping boxes\n",
    "    self.thresh = thresh_area\n",
    "\n",
    "\n",
    "  def _handle_audio_file(self, audio_file, img_dir):\n",
    "    #splits the audio file into minute long spectrograms every 30 seconds\n",
    "\n",
    "    whale_song = AudioSegment.from_wav(audio_file)\n",
    "    song_length = len(whale_song)\n",
    "    pause = half_minute - (song_length % half_minute)\n",
    "\n",
    "    if pause != half_minute:\n",
    "      whale_song = whale_song + AudioSegment.silent(duration=pause)\n",
    "    else:\n",
    "      pause = 0\n",
    "\n",
    "    song_length += pause\n",
    "    prev_minute = 0\n",
    "    \n",
    "    if self.mns is not None: #if the model is trained on spectrally subtracted images\n",
    "      while (prev_minute + minute) <= song_length:\n",
    "        segment = whale_song[prev_minute:(prev_minute + minute)]\n",
    "        self._spectral_subtraction(img_path=img_dir, start=int(prev_minute/1000), end=int((prev_minute + minute)/1000), segment = segment)\n",
    "        prev_minute += half_minute\n",
    "        gc.collect()\n",
    "\n",
    "    else:\n",
    "      while (prev_minute + minute) <= song_length:\n",
    "        segment = whale_song[prev_minute:(prev_minute + minute)]\n",
    "        Annotation_Maker._create_spectrogram(img_path=img_dir, start=int(prev_minute/1000), end=int((prev_minute + minute)/1000), segment = segment)\n",
    "        prev_minute += half_minute\n",
    "\n",
    "  @classmethod \n",
    "  def _create_spectrogram(cls, img_path, start, end, segment = None, samples = None, sample_rate = None):\n",
    "      # generates a spectrogram from a minute long sound segment\n",
    "      if segment:\n",
    "        samples = np.array(segment.get_array_of_samples())\n",
    "        sample_rate = segment.frame_rate\n",
    "\n",
    "      frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "\n",
    "      fig,ax = plt.subplots()\n",
    "      plt.specgram(samples,Fs=sample_rate,NFFT=14705)\n",
    "\n",
    "      plt.axis(ymax=ymax)\n",
    "      ax.set_aspect(1./ax.get_data_ratio())\n",
    "      plt.axis('off')\n",
    "      \n",
    "      fig.set_size_inches(dim,dim, forward=True)\n",
    "\n",
    "      new_file = prefix + str(start) + '_' + str(end)\n",
    "      plt.savefig(img_path + new_file, bbox_inches='tight', pad_inches = 0)\n",
    "      plt.close()\n",
    "\n",
    "\n",
    "  def _spectral_subtraction(self, img_path, start,end, segment):\n",
    "    # spectrally subtracts the minute long sound segment with given noise file before generating an image out of it\n",
    "\n",
    "    w = np.array(segment.get_array_of_samples()).astype(np.float32)/int16_max\n",
    "    sr = segment.frame_rate\n",
    "\n",
    "    s= librosa.stft(w)    # Short-time Fourier transform\n",
    "    angle = np.angle(s)  # get phase\n",
    "\n",
    "    ss = np.abs(s)         # get magnitude\n",
    "    b =np.exp(1.0j* angle) # use this phase information when Inverse Transform\n",
    "\n",
    "    sa = ss - self.mns  \n",
    "    sa0 = sa * b  # apply phase information\n",
    "    y = (librosa.istft(sa0) * int16_max).astype(np.int16) \n",
    "\n",
    "    Annotation_Maker._create_spectrogram(img_path=img_path, start=start, end=end, samples=y, sample_rate=sr)\n",
    "\n",
    "\n",
    "  def _predict_on_images(self, img_array, box_path):\n",
    "    # runs the model on the images generated from _handle_audio_file\n",
    "    # if box_path is defined, exports the drawn image predictions from the model there\n",
    "    preds = self.model.predict(img_array)\n",
    "    if box_path is not None:\n",
    "      if not os.path.exists(box_path):\n",
    "        os.makedirs(box_path)\n",
    "      self.model.export_boxed_images(preds, box_path)\n",
    "    \n",
    "    return preds\n",
    "    \n",
    "  @classmethod\n",
    "  def _process_preds(cls, preds):\n",
    "    #converts predictions to time-frequency domain\n",
    "    for pred in preds:\n",
    "      time_freq_list = []\n",
    "      for box,class_num, score in zip(pred[\"boxes\"], pred[\"classes\"], pred[\"scores\"]):\n",
    "        time_freq_list.append(\n",
    "            cls._convert_to_time_freq(pred[\"image_name\"], box[0], box[1], box[2], box[3], class_num, score)\n",
    "        )\n",
    "      pred[\"time_freq\"] = time_freq_list\n",
    "\n",
    "    return preds\n",
    "\n",
    "  @classmethod\n",
    "  def _convert_to_time_freq(cls, image_name, y0, x0, y1, x1, class_num, score):\n",
    "    # helper function to convert to time-frequency domain\n",
    "    time_start = int(os.path.basename(image_name).split(\"_\")[1])\n",
    "    time_begin = (time_max * x0) + time_start\n",
    "    time_end = (time_max * x1) + time_start\n",
    "\n",
    "    freq_low = frequency_max - (frequency_max * y1) \n",
    "    freq_high = frequency_max - (frequency_max * y0)\n",
    "\n",
    "    return {\n",
    "            \"class\": class_num,\n",
    "            \"time_start\": time_begin,\n",
    "            \"time_end\": time_end,\n",
    "            \"freq_low\":freq_low,\n",
    "            \"freq_high\": freq_high,\n",
    "            \"score\": score\n",
    "           }\n",
    "\n",
    "  @classmethod\n",
    "  def _handle_overlaps(cls, preds, area_thresh):  \n",
    "    # condenses predictions of the same class with at least area_thresh of overlap\n",
    "    # returns a list of dictionaries whose format is defined in _condense_to_pred\n",
    "    pred_list = []\n",
    "\n",
    "    for pred in preds:\n",
    "      for time_freq in pred[\"time_freq\"]:\n",
    "        pred_list.append(time_freq)\n",
    "\n",
    "    pred_list = sorted(pred_list, key= lambda k: k[\"time_start\"]) \n",
    "\n",
    "    return cls._condense_preds(pred_list, area_thresh)\n",
    "    \n",
    "  @classmethod\n",
    "  def _condense_preds(cls, pred_list, area_thresh):\n",
    "    # helper function that condenses predictions of the same class\n",
    "    i = 0\n",
    "    list_len = len(pred_list)\n",
    "    while i + 1 < list_len:\n",
    "      \n",
    "      j = i + 1\n",
    "\n",
    "      while j < list_len and pred_list[j][\"time_start\"] < pred_list[i][\"time_end\"]:\n",
    "        if cls._check_class_and_area(pred_list[i], pred_list[j], area_thresh):\n",
    "          pred_list[i] =  cls._condense_to_pred(pred_list[i], pred_list[j])\n",
    "          del pred_list[j]\n",
    "          list_len -= 1\n",
    "          i-=1\n",
    "          break\n",
    "        else:\n",
    "          j+=1\n",
    "\n",
    "      i += 1\n",
    "    \n",
    "    return pred_list\n",
    "\n",
    "  @classmethod\n",
    "  def _condense_to_pred(cls, A, B):\n",
    "    # helper function to condense 2 predictions\n",
    "    return {\n",
    "        \"class\": A[\"class\"],\n",
    "        \"time_start\": A[\"time_start\"],\n",
    "        \"time_end\": max(A[\"time_end\"],B[\"time_end\"]),\n",
    "        \"freq_low\": min(A[\"freq_low\"],B[\"freq_low\"]),\n",
    "        \"freq_high\": max(A[\"freq_high\"],B[\"freq_high\"]),\n",
    "        \"score\": (A[\"score\"] + B[\"score\"])/2 # averaging the score\n",
    "    }\n",
    "  @classmethod\n",
    "  def _check_class_and_area(cls, pred_A, pred_B, area_thresh):\n",
    "    # boolean function to determine if we should condense two predictions\n",
    "\n",
    "    if pred_A[\"class\"] == pred_B[\"class\"]:\n",
    "      t = pred_A[\"time_end\"] - pred_B[\"time_start\"]\n",
    "      f = min(pred_A[\"freq_high\"], pred_B[\"freq_high\"]) - max(pred_A[\"freq_low\"], pred_B[\"freq_low\"])\n",
    "\n",
    "      shared_area = t * f\n",
    "\n",
    "      A = (pred_A[\"time_end\"] - pred_A[\"time_start\"]) * (pred_A[\"freq_high\"] - pred_A[\"freq_low\"])\n",
    "      B = (pred_B[\"time_end\"] - pred_B[\"time_start\"]) * (pred_B[\"freq_high\"] - pred_B[\"freq_low\"])\n",
    "\n",
    "      return shared_area > 0 and (shared_area/A >= area_thresh or shared_area/B >= area_thresh)\n",
    "    return False\n",
    "\n",
    "  @classmethod\n",
    "  def _make_csv(cls, output_csv_path, pred_list):\n",
    "    # generates the csv after all the predictions have been condensed \n",
    "    # pred_list is a list of dictionaries whose format is defined in _condense_to_pred\n",
    "    rows = []\n",
    "    first_row = [\"Class\", \"Time Start(s)\", \"Time End(s)\", \"Frequency Low(Hz)\", \"Frequency High(Hz)\", \"Delta Time(s)\",\"Delta Frequency(Hz)\", \"Score\"]\n",
    "\n",
    "    rows.append(first_row)\n",
    "    \n",
    "    for time_freq in pred_list:\n",
    "      class_name = class_dict[time_freq[\"class\"]]\n",
    "      begin = time_freq[\"time_start\"]\n",
    "      end = time_freq[\"time_end\"]\n",
    "      low = time_freq[\"freq_low\"]\n",
    "      high = time_freq[\"freq_high\"]\n",
    "      delta_t = end-begin\n",
    "      delta_f = high-low\n",
    "      score = time_freq[\"score\"]\n",
    "      rows.append([class_name, begin, end, low, high, delta_t, delta_f, score])\n",
    "\n",
    "    with open(output_csv_path, \"w+\") as csv_file:\n",
    "      writer = csv.writer(csv_file)\n",
    "      writer.writerows(rows)\n",
    "\n",
    "  def generate_training_images(self, audio_file, image_segment_path):\n",
    "      # generates training images for google cloud vision\n",
    "      # if a noise file is given when initializing the class, it will generate spectrally subtracted images\n",
    "      if not os.path.exists(image_segment_path):\n",
    "        os.makedirs(image_segment_path)\n",
    "      self._handle_audio_file(audio_file, image_segment_path)\n",
    "\n",
    "  def generate_csv(self, audio_path, image_segment_path, output_csv_path, box_path = None):\n",
    "\n",
    "    # generates a csv out of an audio file detecting the whale and other noises in it\n",
    "    # can only be run if tf_path is defined when initializing the class\n",
    "\n",
    "    # the csv is formatted as such: \n",
    "    # \"Class\", \"Time Start(s)\", \"Time End(s)\", \"Frequency Low(Hz)\", \"Frequency High(Hz)\", \"Delta Time(s)\",\"Delta Frequency(Hz)\", \"Score\"\n",
    "\n",
    "    if not os.path.exists(image_segment_path):\n",
    "      os.makedirs(image_segment_path)\n",
    "    self._handle_audio_file(audio_path, image_segment_path)\n",
    "\n",
    "    img_arr = glob.glob(image_segment_path + \"*\")\n",
    "    preds = self._predict_on_images(img_arr, box_path)\n",
    "    del img_arr\n",
    "    gc.collect()\n",
    "\n",
    "    processed_preds = Annotation_Maker._process_preds(preds)\n",
    "    del preds\n",
    "    gc.collect()\n",
    "\n",
    "    processed_preds = Annotation_Maker._handle_overlaps(processed_preds, self.thresh)\n",
    "    Annotation_Maker._make_csv(output_csv_path, processed_preds)\n",
    "    del processed_preds\n",
    "    gc.collect()\n",
    "\n",
    "# additional functions to wrap Annotation_Maker\n",
    "def generate_a_csv(audio_path, tf_path, output_csv_path, thresh_model, thresh_area, image_segment_path, box_path, noise_file):\n",
    "  am = Annotation_Maker(tf_path=tf_path, spectral_subtraction_noise_file=noise_file, thresh_model=thresh_model, thresh_area=thresh_area)\n",
    "  display(\"Beginning to Generate a CSV\")\n",
    "  if image_segment_path is None:\n",
    "    temp_path = \"./tmp/\"\n",
    "    am.generate_csv(audio_path=audio_path, image_segment_path=temp_path, output_csv_path=output_csv_path, box_path=box_path)\n",
    "    shutil.rmtree(temp_path)\n",
    "  else:\n",
    "    am.generate_csv(audio_path=audio_path, image_segment_path=image_segment_path, output_csv_path=output_csv_path, box_path=box_path)\n",
    "  gc.collect()\n",
    "  display(\"Completed\")\n",
    "\n",
    "\n",
    "def generate_the_training_images(audio_path, image_segment_path, noise_file):\n",
    "  display(\"Beginning to Generate Training Images\")\n",
    "  am = Annotation_Maker(spectral_subtraction_noise_file= noise_file)\n",
    "  am.generate_training_images(audio_path, image_segment_path)\n",
    "  gc.collect()\n",
    "  display(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufCBJvZYKEIV"
   },
   "source": [
    "### **Generate CSV**\n",
    "\n",
    "Run the following cell to generate a csv with the following format:  \n",
    "`Class, Time Start(s), Time End(s), Frequency Low(Hz), Frequency High(Hz), Delta Time(s), Delta Frequency(Hz), Score`\n",
    "\n",
    "Modify the following values in the cells to control what the following code does\n",
    "\n",
    "#### *General User modifiable parameters:*\n",
    "\n",
    "`audio_path`: The path to where the audio file that you want to detect whale sounds on is located\n",
    "\n",
    "`tf_path`: The path to the `tflite` model.\n",
    "\n",
    "`output_csv_path`:The path of the csv you want to create\n",
    "\n",
    "`thresh_model`(range 0 to 1 - should be in decimals): The minimum percentage of confidence that the prediction is correctly detecting something \n",
    "\n",
    "`thresh_area`(range 0 to 1 - should be in decimals): The minimum percentage of area of overlap between two overlapping predictions to be treated as a single prediction.\n",
    "\n",
    "`image_segment_path` (default value is `None`): If given `None`, the code will generate images for the model to predict on and deletes them. If given a path to a directory, it will create that directory if it doesn't exist and won't delete the images stored there after it generates the csv.  \n",
    "**Important:** Make sure your path ends in a **/** if you give it a directory. \n",
    "\n",
    "`box_path` (default value is `None`): If given `None`, the code will not generate images with the initial unprocessed predictions given by the `tflite_model`. If given a path, those images will be generated and stored at that path. Giving this value a path typically adds 1-3 minutes to the total run time on Google Colab.\n",
    "**Important:** Make sure your path ends in a **/** if you give it a directory. \n",
    "\n",
    "\n",
    "#### *Spectral Subtraction specific modifiers:*\n",
    "\n",
    "**Note**: Spectral Subtraction causes this code to run for twice as long with minimal improvement for our current models.\n",
    "\n",
    "`noise_file` (default value is `None`): The path to wehere the background noise file used in spectral subtraction will be located. Do not set this value if you are not using a model trained on spectrally subtracted images. If given `None`, assumes spectral subtraction won't be used. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcSwWbVxKDE7"
   },
   "outputs": [],
   "source": [
    "\n",
    "audio_path = \"/content/whale_song_segment_0_10.wav\"\n",
    "tf_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/Models/Model6/Model6_model-export_iod_tflite-Model6-2021-05-30T05_10_46.676148Z_model.tflite\"\n",
    "output_csv_path = \"small_test_s.csv\"\n",
    "\n",
    "thresh_model = 0.5\n",
    "thresh_area = 0.5\n",
    "\n",
    "image_segment_path = None\n",
    "box_path = None\n",
    "\n",
    "noise_file = None\n",
    "\n",
    "generate_a_csv(audio_path, tf_path, output_csv_path, thresh_model, thresh_area, image_segment_path, box_path, noise_file)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzTAkFHTPtA1"
   },
   "source": [
    "### **Generate Training Images**\n",
    "\n",
    "Run the following cell to generate training images to use in Google Cloud Vision. Make sure the `image_segment_path` is a place in your Google Drive so you can download the images easily.\n",
    "\n",
    "#### *General User modifiable parameters:*\n",
    "\n",
    "`audio_path`: The path to where the audio file that you want to use for training to be located\n",
    "\n",
    "`image_segment_path`: The path to where your training images will be located\n",
    "\n",
    "**Important:** Make sure your path ends in a **/** if you give it a directory. \n",
    "\n",
    "#### *Spectral Subtraction specific modifiers:*\n",
    "\n",
    "`noise_file` (default value is `None`): The path to wehere the background noise file used in spectral subtraction will be located. Only set this if you want to create spectrally subtracted models from Google Cloud Vision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGbq4uy64NVr"
   },
   "outputs": [],
   "source": [
    "# remove the # of the lines below to run this code\n",
    "# audio_path = \"/content/whale_song_segment_0_10.wav\"\n",
    "# image_segment_path = \"/content/test_dir_small/\"\n",
    "\n",
    "# noise_file = None\n",
    "\n",
    "# generate_the_training_images(audio_path, image_segment_path, noise_file)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljKLn8ldWG2H"
   },
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO1qZSDGr8Bj"
   },
   "source": [
    "#### Generating CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnI-QpfJr-s1"
   },
   "source": [
    "##### Small File (11 Minutes), No spectral Subtraction, No Prediction Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "dHbOz5OBxNPv",
    "outputId": "3fb4157e-59e3-420c-cd42-cf8df48ff69e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Beginning to Generate a CSV'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Completed'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.9 s, sys: 213 ms, total: 36.1 s\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# audio_path = \"/content/whale_song_segment_0_10.wav\"\n",
    "# tf_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/Models/Model5/Model_No_Spectral_Subtraction.tflite\" \n",
    "# output_csv_path = \"small_test_no_s_no_p.csv\"\n",
    "\n",
    "# thresh_model = 0.5\n",
    "# thresh_area = 0.5\n",
    "\n",
    "# image_segment_path = None\n",
    "# box_path = None\n",
    "# noise_file = None\n",
    "\n",
    "# generate_a_csv(audio_path, tf_path, output_csv_path, thresh_model, thresh_area, image_segment_path, box_path, noise_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU-KEsFUs6Th"
   },
   "source": [
    "##### Small File (11 Minutes), No spectral Subtraction, With Prediction Outputs\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "_8OGCtqDxOPI",
    "outputId": "6b0ee5b9-1deb-4d94-f718-94041789b8c7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Beginning to Generate a CSV'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Completed'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.7 s, sys: 262 ms, total: 40.9 s\n",
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# audio_path = \"/content/whale_song_segment_0_10.wav\"\n",
    "# tf_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/Models/Model5/Model_No_Spectral_Subtraction.tflite\" \n",
    "# output_csv_path = \"small_test_no_s_w_p.csv\"\n",
    "\n",
    "# thresh_model = 0.5\n",
    "# thresh_area = 0.5\n",
    "\n",
    "# image_segment_path = None\n",
    "# box_path = \"/content/box_dir/\"\n",
    "# noise_file = None\n",
    "\n",
    "# generate_a_csv(audio_path, tf_path, output_csv_path, thresh_model, thresh_area, image_segment_path, box_path, noise_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwcp3yeYtCPx"
   },
   "source": [
    "##### Small File (11 Minutes), Spectral Subtraction, No Prediction Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "u04iLmrNxPGx",
    "outputId": "6a3258f6-eb5e-40b0-8e19-27dd078ca70c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Beginning to Generate a CSV'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Completed'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 2.4 s, total: 1min 23s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# audio_path = \"/content/whale_song_segment_0_10.wav\"\n",
    "# tf_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/Models/Model6/Model_Spectral_Subtraction.tflite\"\n",
    "# output_csv_path = \"small_test_w_s_no_p.csv\"\n",
    "\n",
    "# thresh_model = 0.5\n",
    "# thresh_area = 0.5\n",
    "\n",
    "# image_segment_path = None\n",
    "# box_path = None\n",
    "# noise_file = \"/content/drive/MyDrive/Capstone - Whale Sounds/Matt-Bounding-Box-Work/Sounds/Original/noise.wav\"\n",
    "\n",
    "# generate_a_csv(audio_path, tf_path, output_csv_path, thresh_model, thresh_area, image_segment_path, box_path, noise_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UQwdt-ftbhq"
   },
   "source": [
    "##### Small File (11 Minutes), Spectral Subtraction, With Prediction Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "zxMt1ETXxP_F",
    "outputId": "5d62a63f-f879-495d-f9e5-d90c2b7cdb61"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Beginning to Generate a CSV'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Completed'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 955 ms, total: 1min 25s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# audio_path = \"/content/whale_song_segment_0_10.wav\"\n",
    "# tf_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/Models/Model6/Model_Spectral_Subtraction.tflite\"\n",
    "# output_csv_path = \"small_test_w_s_w_p.csv\"\n",
    "\n",
    "# thresh_model = 0.5\n",
    "# thresh_area = 0.5\n",
    "\n",
    "# image_segment_path = None\n",
    "# box_path = \"/content/box_dir/\"\n",
    "# noise_file = \"/content/drive/MyDrive/Capstone - Whale Sounds/Matt-Bounding-Box-Work/Sounds/Original/noise.wav\"\n",
    "\n",
    "# generate_a_csv(audio_path, tf_path, output_csv_path, thresh_model, thresh_area, image_segment_path, box_path, noise_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRA3fHSStbJU"
   },
   "source": [
    "##### Large File (approx. 3hrs), No Spectral Subtraction, No Prediction Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "8NRVpv2jxWW9",
    "outputId": "3f2ddd2d-b4a8-43e7-a45f-c2971a48d53b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Beginning to Generate a CSV'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Completed'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 40s, sys: 24.3 s, total: 11min 4s\n",
      "Wall time: 11min 39s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# audio_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/amp_671658014.180929033558.wav\"\n",
    "# tf_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/Models/Model5/Model_No_Spectral_Subtraction.tflite\"\n",
    "# output_csv_path = \"large_test_no_s_no_p.csv\"\n",
    "\n",
    "# thresh_model = 0.5\n",
    "# thresh_area = 0.5\n",
    "\n",
    "# image_segment_path = None\n",
    "# box_path = None\n",
    "# noise_file = None\n",
    "\n",
    "# generate_a_csv(audio_path, tf_path, output_csv_path, thresh_model, thresh_area, image_segment_path, box_path, noise_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cKFUzFCs_Gx"
   },
   "source": [
    "##### Large File (approx. 3hrs),  No spectral Subtraction, With Prediction Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "epU_65uVxYjl",
    "outputId": "3b4dfe8e-05f3-4c1b-9521-b00eeb2d72ec"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Beginning to Generate a CSV'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py:7592: RuntimeWarning: divide by zero encountered in log10\n",
      "  Z = 10. * np.log10(spec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Completed'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 3s, sys: 51.8 s, total: 12min 55s\n",
      "Wall time: 13min 37s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# audio_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/amp_671658014.180929033558.wav\"\n",
    "# tf_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/Models/Model5/Model_No_Spectral_Subtraction.tflite\"\n",
    "# output_csv_path = \"large_test_no_s_w_p.csv\"\n",
    "\n",
    "# thresh_model = 0.5\n",
    "# thresh_area = 0.5\n",
    "\n",
    "# image_segment_path = None\n",
    "# box_path = \"/content/box_dir/\"\n",
    "# noise_file = None\n",
    "\n",
    "# generate_a_csv(audio_path, tf_path, output_csv_path, thresh_model, thresh_area, image_segment_path, box_path, noise_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU3f2fYRt6zT"
   },
   "source": [
    "\n",
    "##### Large File (approx. 3hrs), Spectral Subtraction, No Prediction Outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "4uLJGjESxZpO",
    "outputId": "356ca19d-44b1-4fe4-bf21-ca39ed7faf25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Beginning to Generate a CSV'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Completed'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 1s, sys: 23.7 s, total: 21min 24s\n",
      "Wall time: 22min 4s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# audio_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/amp_671658014.180929033558.wav\"\n",
    "# tf_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/Models/Model6/Model_Spectral_Subtraction.tflite\"\n",
    "# output_csv_path = \"large_test_w_s_no_p.csv\"\n",
    "\n",
    "# thresh_model = 0.5\n",
    "# thresh_area = 0.5\n",
    "\n",
    "# image_segment_path = None\n",
    "# box_path = None\n",
    "# noise_file = \"/content/drive/MyDrive/Capstone - Whale Sounds/Matt-Bounding-Box-Work/Sounds/Original/noise.wav\"\n",
    "\n",
    "# generate_a_csv(audio_path, tf_path, output_csv_path, thresh_model, thresh_area, image_segment_path, box_path, noise_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_Vpbdoat64t"
   },
   "source": [
    "##### Large File (approx. 3hrs), Spectral Subtraction, With Prediction Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Os-KVQmvvP_6",
    "outputId": "cdb140b1-45a7-4ebf-ba1e-5a1c0524319c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Beginning to Generate a CSV'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Completed'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 16s, sys: 29.3 s, total: 22min 45s\n",
      "Wall time: 23min 24s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# audio_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/amp_671658014.180929033558.wav\"\n",
    "# tf_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/Models/Model6/Model_Spectral_Subtraction.tflite\"\n",
    "# output_csv_path = \"large_test_w_s_w_p.csv\"\n",
    "\n",
    "# thresh_model = 0.5\n",
    "# thresh_area = 0.5\n",
    "\n",
    "# image_segment_path = None\n",
    "# box_path = \"/content/box_dir/\"\n",
    "# noise_file = \"/content/drive/MyDrive/Capstone - Whale Sounds/Matt-Bounding-Box-Work/Sounds/Original/noise.wav\"\n",
    "\n",
    "# generate_a_csv(audio_path, tf_path, output_csv_path, thresh_model, thresh_area, image_segment_path, box_path, noise_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHFS3YKOnlVB"
   },
   "source": [
    "### Generating Training Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cC-IEdSFnuV_"
   },
   "source": [
    "#### Large File (approx. 3hrs), No Spectral Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "oNx9QTzNnr5p",
    "outputId": "d53cb783-2a0f-420e-e005-8f35bfa06e10"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Beginning to Generate Training Images'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Completed'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 47s, sys: 24.6 s, total: 8min 12s\n",
      "Wall time: 8min 34s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# audio_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/amp_671658014.180929033558.wav\"\n",
    "# image_segment_path = \"/content/test_dir/\"\n",
    "\n",
    "# noise_file = None\n",
    "\n",
    "# generate_the_training_images(audio_path, image_segment_path, noise_file)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSj3FA0rn3Jh"
   },
   "source": [
    "#### Large File (approx. 3hrs), Spectral Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "omafMXE9n-GB",
    "outputId": "d0c8400e-ab0d-4ef5-ec27-4e2de8a35ad2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Beginning to Generate Training Images'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Completed'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 43s, sys: 37.1 s, total: 20min 20s\n",
      "Wall time: 20min 59s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# audio_path = \"/content/drive/MyDrive/Capstone - Whale Sounds/amp_671658014.180929033558.wav\"\n",
    "# image_segment_path = \"/content/test_dir/\"\n",
    "\n",
    "# noise_file = \"/content/drive/MyDrive/Capstone - Whale Sounds/Matt-Bounding-Box-Work/Sounds/Original/noise.wav\"\n",
    "\n",
    "# generate_the_training_images(audio_path, image_segment_path, noise_file)\n",
    "# gc.collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final_TFLITE_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
