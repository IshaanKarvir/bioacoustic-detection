{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.io import wavfile\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# For producing label map for TF2 Obj Detection\n",
    "from object_detection.protos import string_int_label_map_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "# For saving TFRecords\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util\n",
    "import io\n",
    "\n",
    "# For sharding TFRecords\n",
    "import contextlib2\n",
    "from object_detection.dataset_tools import tf_record_creation_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_col, right_col = \"Begin Time (s)\", \"End Time (s)\"\n",
    "top_col, bot_col = \"High Freq (Hz)\", \"Low Freq (Hz)\"\n",
    "class_col, class_conf_col = \"Species\", \"Species Confidence\"\n",
    "\n",
    "recording_dir = \"../data\"\n",
    "annotation_dir = \"../data\"\n",
    "output_dir = \"../data/spectrograms-1600Hz-15s-300px-082621\"\n",
    "label_map_name = \"label_map.pbtxt\"\n",
    "metadata_name = \"dataset_metadata.txt\"\n",
    "\n",
    "# SPECTROGRAM CONSTANTS\n",
    "# Window size (n_fft) in seconds\n",
    "WINDOW_SIZE_SEC = 3/20\n",
    "# Hop Length in seconds\n",
    "HOP_LEN_SEC = 15/300\n",
    "# Number of frequency bands (y dimension of spectrogram)\n",
    "N_MELS = 300\n",
    "# Maximum frequency considered (highest value in y dimension)\n",
    "FREQUENCY_MAX = 1600\n",
    "\n",
    "# CHUNK CONSTANTS\n",
    "# Length of one chunk in seconds\n",
    "TRAIN_CHUNK_SIZE_SEC = 45\n",
    "EVAL_CHUNK_SIZE_SEC = 15\n",
    "# Minimum % visibility of a call to keep annotation\n",
    "MIN_BOX_PERCENT = 0.3\n",
    "\n",
    "# DATASET SETTINGS\n",
    "dataset_name = \"dataset.record\"\n",
    "NUM_TRAIN_SHARDS = 3\n",
    "NUM_EVAL_SHARDS = 5\n",
    "NUM_EVAL_FILES = 2\n",
    "\n",
    "# Constructs the dataset without certain classes\n",
    "DISALLOWED_CLASSES = [\"?\", \"rf\", \"sl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory is: '/home/jackson/Projects/marine-acoustics-2021/data/spectrograms-1600Hz-15s-300px-082621'\n",
      "Successfully created directory.\n"
     ]
    }
   ],
   "source": [
    "output_dir = path.abspath(output_dir)\n",
    "print(\"Output directory is: '{}'\".format(output_dir))\n",
    "\n",
    "os.makedirs(output_dir)\n",
    "print(\"Successfully created directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join(output_dir, metadata_name), 'w') as metafile:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"WINDOW_SIZE_SEC\": WINDOW_SIZE_SEC,\n",
    "            \"HOP_LEN_SEC\": HOP_LEN_SEC,\n",
    "            \"N_MELS\": N_MELS,\n",
    "            \"FREQUENCY_MAX\": FREQUENCY_MAX,\n",
    "            \"TRAIN_CHUNK_SIZE_SEC\": TRAIN_CHUNK_SIZE_SEC,\n",
    "            \"EVAL_CHUNK_SIZE_SEC\": EVAL_CHUNK_SIZE_SEC,\n",
    "            \"EVAL_CHUNK_STEP_SEC\": EVAL_CHUNK_SIZE_SEC / 2.0\n",
    "        },\n",
    "        metafile\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_id(fname):\n",
    "    return path.basename(fname)[:22]\n",
    "\n",
    "# Constructs a list of pairs (wav_fname, annot_fname)\n",
    "def get_filename_pairs(recording_dir, annotation_dir):\n",
    "    wav_fnames = glob.glob(path.join(recording_dir, \"*.wav\"))\n",
    "    annot_fnames = glob.glob(path.join(annotation_dir, \"*.txt\"))\n",
    "    \n",
    "    res = []\n",
    "    id_set = set()\n",
    "    for wfname in wav_fnames:\n",
    "        id_nums = get_file_id(wfname)\n",
    "        if id_nums in id_set:\n",
    "            raise ValueError(\"Duplicate Wav ID: {}\".format(id_nums))\n",
    "        id_set.add(id_nums)\n",
    "        \n",
    "        annots = [a for a in annot_fnames if path.basename(a).startswith(id_nums)]\n",
    "        if len(annots) > 1:\n",
    "            raise ValueError(\"More than one annotation for recording: {}\".format(wfname))\n",
    "        if len(annots) < 1:\n",
    "            print(\"No annotation for recording: {}\".format(wfname))\n",
    "            continue\n",
    "        \n",
    "        res.append((wfname, annots[0]))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No annotation for recording: ../data/671658014.181001003530_norm_8k-resample.wav\n",
      "No annotation for recording: ../data/671658014.180930213531_norm_8k-resample.wav\n",
      "No annotation for recording: ../data/671658014.181001033528_norm_8k-resample.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('../data/671658014.180930063538_norm_8k-resample.wav',\n",
       "  '../data/671658014.180930063538-MS.txt'),\n",
       " ('../data/671658014.180930003542_norm_8k-resample.wav',\n",
       "  '../data/671658014.180930003542-AW.txt'),\n",
       " ('../data/671658014.180930033540_norm_8k-resample.wav',\n",
       "  '../data/671658014.180930033540-AW.txt'),\n",
       " ('../data/671658014.180929003601_norm_8k-resample.wav',\n",
       "  '../data/671658014.180929003601-MS.txt'),\n",
       " ('../data/671658014.180929183547_norm_8k-resample.wav',\n",
       "  '../data/671658014.180929183547-MS.txt'),\n",
       " ('../data/671658014.180930123535_norm_8k-resample.wav',\n",
       "  '../data/671658014.180930123535-MS.txt'),\n",
       " ('../data/671658014.180928183606_norm_8k-resample.wav',\n",
       "  '../data/671658014.180928183606-AW.txt'),\n",
       " ('../data/671658014.180929123551_norm_8k-resample.wav',\n",
       "  '../data/671658014.180929123551-AW.txt'),\n",
       " ('../data/671658014.180929213545_norm_8k-resample.wav',\n",
       "  '../data/671658014.180929213545-AW.txt'),\n",
       " ('../data/671658014.180928213604_norm_8k-resample.wav',\n",
       "  '../data/671658014.180928213604-MS.txt'),\n",
       " ('../data/671658014.180930153534_norm_8k-resample.wav',\n",
       "  '../data/671658014.180930153534-AW.txt'),\n",
       " ('../data/671658014.180929063556_norm_8k-resample.wav',\n",
       "  '../data/671658014.180929063556-MS.txt'),\n",
       " ('../data/671658014.180929093553_norm_8k-resample.wav',\n",
       "  '../data/671658014.180929093553-MS.txt'),\n",
       " ('../data/671658014.180928153609_norm_8k-resample.wav',\n",
       "  '../data/671658014.180928153609-JW.txt'),\n",
       " ('../data/671658014.180930183532_norm_8k-resample.wav',\n",
       "  '../data/671658014.180930183532-MS.txt'),\n",
       " ('../data/671658014.180929033558_norm_8k-resample.wav',\n",
       "  '../data/671658014.180929033558-JW.txt'),\n",
       " ('../data/671658014.180929153549_norm_8k-resample.wav',\n",
       "  '../data/671658014.180929153549-MS.txt')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_filename_pairs(recording_dir, annotation_dir)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_indices = np.random.choice(len(dataset), NUM_EVAL_FILES, replace=False)\n",
    "#eval_indices = np.array([0, 2, 3, 4, 5, 6])\n",
    "\n",
    "#train_dataset = [e for i,e in enumerate(dataset) if i not in eval_indices]\n",
    "#eval_dataset = [e for i,e in enumerate(dataset) if i in eval_indices]\n",
    "\n",
    "#print(\"{} Train files and {} Eval files\". format(len(train_dataset), len(eval_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually Set\n",
    "\n",
    "train_dataset = [\n",
    "    ('../data/671658014.180929033558_norm_8k-resample.wav', '../data/671658014.180929033558-JW.txt'),\n",
    "    ('../data/671658014.180929003601_norm_8k-resample.wav', '../data/671658014.180929003601-MS.txt'),\n",
    "    ('../data/671658014.180928153609_norm_8k-resample.wav', '../data/671658014.180928153609-JW.txt'),\n",
    "    ('../data/671658014.180929123551_norm_8k-resample.wav', '../data/671658014.180929123551-AW.txt'),\n",
    "    ('../data/671658014.180930003542_norm_8k-resample.wav', '../data/671658014.180930003542-AW.txt'),\n",
    "    ('../data/671658014.180930063538_norm_8k-resample.wav', '../data/671658014.180930063538-MS.txt'),\n",
    "    ('../data/671658014.180930033540_norm_8k-resample.wav', '../data/671658014.180930033540-AW.txt'),\n",
    "    ('../data/671658014.180930123535_norm_8k-resample.wav', '../data/671658014.180930123535-MS.txt'),\n",
    "    ('../data/671658014.180929213545_norm_8k-resample.wav', '../data/671658014.180929213545-AW.txt'),\n",
    "    ('../data/671658014.180930183532_norm_8k-resample.wav', '../data/671658014.180930183532-MS.txt'),\n",
    "    ('../data/671658014.180929153549_norm_8k-resample.wav', '../data/671658014.180929153549-MS.txt')\n",
    "]\n",
    "\n",
    "eval_dataset = [\n",
    "    ('../data/671658014.180928183606_norm_8k-resample.wav', '../data/671658014.180928183606-AW.txt'),\n",
    "    ('../data/671658014.180928213604_norm_8k-resample.wav', '../data/671658014.180928213604-MS.txt'),\n",
    "    ('../data/671658014.180929063556_norm_8k-resample.wav', '../data/671658014.180929063556-MS.txt'),\n",
    "    ('../data/671658014.180929093553_norm_8k-resample.wav', '../data/671658014.180929093553-MS.txt'),\n",
    "    ('../data/671658014.180929183547_norm_8k-resample.wav', '../data/671658014.180929183547-MS.txt'),\n",
    "    ('../data/671658014.180930153534_norm_8k-resample.wav', '../data/671658014.180930153534-AW.txt')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: measure speed of different fns for opening wav files\n",
    "def read_wavfile(wav_name, normalize=True, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Reading {}\".format(wav_name))\n",
    "    sr, data = wavfile.read(wav_name)\n",
    "    if verbose:\n",
    "        print(\"{} samples at {} samples/sec --> {} seconds\".format(data.shape[0], sr, data.shape[0]/sr))\n",
    "\n",
    "    if normalize:\n",
    "        data = data.astype(float)\n",
    "        data = data - data.min()\n",
    "        data = data / data.max()\n",
    "        data = data - 0.5\n",
    "    \n",
    "    return sr, data\n",
    "\n",
    "\n",
    "def read_annotations(fname, verbose=False):\n",
    "    annotations = pd.read_csv(fname, sep=\"\\t\")\n",
    "    if verbose:\n",
    "        print(\"Read {} annotations from {}\".format(len(annotations), fname))\n",
    "        print(\"Columns:\", \",\".join([\" {} ({})\".format(c, type(c)) for c in annotations.columns]))\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['?', 'hb', 'rf', 'sl']\n"
     ]
    }
   ],
   "source": [
    "def get_all_classes(annotation_fnames, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns a list of all classes seen in the annotation files sorted\n",
    "    alphabetically.\n",
    "    \"\"\"\n",
    "    classes = set()\n",
    "    for annot_fname in annotation_fnames:\n",
    "        classes.update(list(read_annotations(annot_fname)[class_col].unique()))\n",
    "    classes = sorted([s for s in list(classes)])\n",
    "    if verbose:\n",
    "        print(\"Classes: \", classes)\n",
    "    return classes\n",
    "\n",
    "\n",
    "# Generates the necessary prototext file for the class mapping.\n",
    "# Classes are assigned to the integer 1 greater than their index.\n",
    "# The resulting file is saved to output_path.\n",
    "def create_label_map(classes, output_path):\n",
    "    label_map = string_int_label_map_pb2.StringIntLabelMap()\n",
    "    for i, cls in enumerate(classes):\n",
    "        new_item = label_map.item.add() # StringIntLabelMapItem\n",
    "        new_item.name = cls          # String name. The most common practice is to set this to a MID or synsets id.\n",
    "        new_item.id = 1+i            # Integer id starting from 1\n",
    "        new_item.display_name = cls  # Human readable text label\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(text_format.MessageToString(label_map))\n",
    "        \n",
    "\n",
    "classes = get_all_classes([a for _,a in dataset], verbose=True)\n",
    "classes = [c for c in classes if c not in DISALLOWED_CLASSES]\n",
    "create_label_map(classes, os.path.join(output_dir, label_map_name))\n",
    "\n",
    "\n",
    "# The class and reverse class maps are used to encode classes later.\n",
    "class_map = {}\n",
    "rev_class_map = {}\n",
    "for i in range(len(classes)):\n",
    "    class_map[i+1] = classes[i]\n",
    "    rev_class_map[classes[i]] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(annotation):\n",
    "    return ((annotation[right_col] - annotation[left_col])\n",
    "            * (annotation[top_col] - annotation[bot_col]))\n",
    "\n",
    "\n",
    "# Per-channel energy normalization\n",
    "def PCEN(spec, M_return_timestep, init_val=None, epsilon=1e-6, s=0.001, alpha=0.80, delta=2.0, r=0.5):\n",
    "    output = np.zeros_like(spec)\n",
    "    if M_return_timestep < 0 or M_return_timestep > spec.shape[1]-1:\n",
    "        print(\"Warning! M return timestep is outside bounds. Not returning any M.\")\n",
    "    if init_val is None:\n",
    "        M = np.zeros(shape=(output.shape[0]))\n",
    "    else:\n",
    "        M = np.array(init_val)\n",
    "    assert M.shape[0] == output.shape[0]\n",
    "    out_M = None\n",
    "    for t in range(output.shape[1]):\n",
    "        M = (1 - s) * M + s * spec[:,t]\n",
    "        output[:,t] = ((spec[:,t] / ((M + epsilon) ** alpha)) ** r) - (delta ** r)\n",
    "        if t == M_return_timestep:\n",
    "            out_M = M\n",
    "    return output, out_M\n",
    "\n",
    "\n",
    "# Returns the min and max db observed in all wav files\n",
    "def get_minmax_bounds(wav_filenames, chunk_size=TRAIN_CHUNK_SIZE_SEC):\n",
    "    min_val, max_val = None, None\n",
    "    for wfname in wav_filenames:\n",
    "        sr, data = read_wavfile(wfname, normalize=True)\n",
    "        n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "        hop_len = int(HOP_LEN_SEC * sr)\n",
    "        chunk_size = int(chunk_size * sr)\n",
    "        step = chunk_size - (hop_len * (N_MELS-2) + n_fft)\n",
    "        M_init = None\n",
    "        for start_i in range(0, len(data), step):\n",
    "            mel_spec = librosa.feature.melspectrogram(y=data[start_i:min(len(data),start_i+chunk_size)],\n",
    "                                                      sr=sr,\n",
    "                                                      n_fft=n_fft,\n",
    "                                                      hop_length=hop_len,\n",
    "                                                      n_mels=N_MELS,\n",
    "                                                      fmax=FREQUENCY_MAX,\n",
    "                                                      center=False)\n",
    "            #mel_spec, M_init = PCEN(mel_spec, step // hop_len, init_val=M_init)\n",
    "            mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            temp_min = mel_spec.min()\n",
    "            temp_max = mel_spec.max()\n",
    "            if min_val is None or temp_min < min_val:\n",
    "                min_val = temp_min\n",
    "            if max_val is None or temp_max > max_val:\n",
    "                max_val = temp_max\n",
    "    return min_val, max_val\n",
    "\n",
    "\n",
    "# Creates the tf.train.Example from the example_dict saved for each spectrogram chunk\n",
    "# The image bytes are read in from disk at this point.\n",
    "def create_tf_example(example_dict):\n",
    "    with open(example_dict[\"filepath\"], \"rb\") as f:\n",
    "        encoded_image_data = f.read()\n",
    "    filename = path.basename(example_dict[\"filepath\"]).encode()\n",
    "    classes_text = [s.encode() for s in example_dict[\"classes_text\"]]\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "              'image/height': dataset_util.int64_feature(example_dict[\"height\"]),\n",
    "              'image/width': dataset_util.int64_feature(example_dict[\"width\"]),\n",
    "              'image/filename': dataset_util.bytes_feature(filename),\n",
    "              'image/source_id': dataset_util.bytes_feature(filename),\n",
    "              'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
    "              'image/format': dataset_util.bytes_feature(b'png'),\n",
    "              'image/object/bbox/xmin': dataset_util.float_list_feature(example_dict[\"xmins\"]),\n",
    "              'image/object/bbox/xmax': dataset_util.float_list_feature(example_dict[\"xmaxs\"]),\n",
    "              'image/object/bbox/ymin': dataset_util.float_list_feature(example_dict[\"ymins\"]),\n",
    "              'image/object/bbox/ymax': dataset_util.float_list_feature(example_dict[\"ymaxs\"]),\n",
    "              'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "              'image/object/class/label': dataset_util.int64_list_feature(example_dict[\"classes\"]),\n",
    "          }))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "def process_file(wav_filename, annot_filename, min_bound, max_bound, chunk_size, chunk_layout=\"dense\",\n",
    "                 drop_last_chunk=False, verbose=False):\n",
    "    sr, data = read_wavfile(wav_filename, normalize=True, verbose=verbose)\n",
    "    annotations = read_annotations(annot_filename, verbose=verbose)\n",
    "    file_id = get_file_id(wav_filename)\n",
    "    \n",
    "    n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "    hop_len = int(HOP_LEN_SEC * sr)\n",
    "    chunk_size = int(chunk_size * sr)\n",
    "    \n",
    "    if chunk_layout == \"dense\":\n",
    "        step = chunk_size - (hop_len * (N_MELS-2) + n_fft)\n",
    "    elif chunk_layout == \"sparse\":\n",
    "        step = chunk_size // 2\n",
    "    \n",
    "    # Start Indices of each chunk\n",
    "    start_vals = [s for s in range(0, len(data), step)]\n",
    "    \n",
    "    # If last cut point creates a tiny chunk, remove it\n",
    "    if len(data) - start_vals[-1] < int(chunk_size / 2):\n",
    "        start_vals = start_vals[:-1]\n",
    "        \n",
    "    def extract_chunk(start_i, end_i, spec_name, annot_name, use_pcen=True, M_init=None):\n",
    "        mel_spec = librosa.feature.melspectrogram(y=data[start_i:end_i],\n",
    "                                                  sr=sr,\n",
    "                                                  n_fft=n_fft,\n",
    "                                                  hop_length=hop_len,\n",
    "                                                  n_mels=N_MELS,\n",
    "                                                  fmax=FREQUENCY_MAX,\n",
    "                                                  center=False)\n",
    "        #mel_spec, next_M_init = PCEN(mel_spec, step // hop_len, init_val=M_init)\n",
    "        next_M_init = None\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        mel_spec = np.clip((mel_spec - min_bound) / (max_bound - min_bound) * 255, a_min=0, a_max=255)\n",
    "        mel_spec = mel_spec.astype(np.uint8)\n",
    "        spec_height, spec_width = mel_spec.shape\n",
    "        \n",
    "        # Get annotations to those inside chunk\n",
    "        start_s, end_s = start_i/sr, end_i/sr\n",
    "        freq_axis_low, freq_axis_high = librosa.hz_to_mel(0.0), librosa.hz_to_mel(FREQUENCY_MAX)\n",
    "        chunk_annotations = annotations.loc[~((annotations[left_col] > end_s)\n",
    "                                              | (annotations[right_col] < start_s))].copy()\n",
    "        # Rescale axes to 0.0-1.0 based on location inside chunk\n",
    "        chunk_annotations.loc[:,[left_col,right_col]] = ((chunk_annotations[[left_col,right_col]]\n",
    "                                                         - start_s) / (end_s - start_s))\n",
    "        chunk_annotations.loc[:,[bot_col,top_col]] = (1.0 - ((librosa.hz_to_mel(chunk_annotations[[bot_col,top_col]])\n",
    "                                                      - freq_axis_low) / (freq_axis_high - freq_axis_low)))\n",
    "        chunk_annotations = chunk_annotations.loc[chunk_annotations[class_col].isin(classes)]\n",
    "        trimmed_annots = chunk_annotations.copy()\n",
    "        trimmed_annots[left_col] = trimmed_annots[left_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[right_col] = trimmed_annots[right_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[bot_col] = trimmed_annots[bot_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[top_col] = trimmed_annots[top_col].clip(lower=0, upper=1.0)\n",
    "        overlaps = []\n",
    "        for i in trimmed_annots.index:\n",
    "            intersection = trimmed_annots.loc[i]\n",
    "            original = chunk_annotations.loc[i]\n",
    "            original_area = get_area(original)\n",
    "            overlaps.append((get_area(intersection)*spec_height*spec_width) / original_area)\n",
    "        chunk_annotations = trimmed_annots.loc[np.array(overlaps) > MIN_BOX_PERCENT]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Found {} annotations in chunk\".format(len(chunk_annotations)))\n",
    "        \n",
    "        # Save Chunk as PNG image (lossless compression)\n",
    "        im = Image.fromarray(mel_spec[::-1, :])\n",
    "        im = im.convert(\"L\")\n",
    "        image_filepath = path.join(output_dir, spec_name)\n",
    "        im.save(image_filepath)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Saved spectrogram to '{}'\".format(spec_name))\n",
    "        \n",
    "        example_dict = {\n",
    "            \"filepath\": image_filepath,\n",
    "            \"height\": spec_height,\n",
    "            \"width\": spec_width,\n",
    "            \"xmins\": trimmed_annots[left_col].tolist(),\n",
    "            \"xmaxs\": trimmed_annots[right_col].tolist(),\n",
    "            \"ymins\": trimmed_annots[top_col].tolist(),\n",
    "            \"ymaxs\": trimmed_annots[bot_col].tolist(),\n",
    "            \"classes_text\": trimmed_annots[class_col].tolist(),\n",
    "            \"classes\": trimmed_annots[class_col].map(rev_class_map).tolist()\n",
    "        }\n",
    "        return example_dict, next_M_init\n",
    "    \n",
    "    \n",
    "    # Actually iterate through the file and extract chunks\n",
    "    examples = []\n",
    "    M_init = None\n",
    "    for ind, start_i in enumerate(start_vals[:-1]):\n",
    "        spec_name = \"{}-{}.png\".format(file_id, ind)\n",
    "        annot_name = \"{}-{}-labels.txt\".format(file_id, ind)\n",
    "        ex, M_init = extract_chunk(start_i, start_i+chunk_size, spec_name, annot_name, M_init=M_init)\n",
    "        examples.append(ex)\n",
    "    if not drop_last_chunk:\n",
    "        spec_name = \"{}-{}.png\".format(file_id, len(start_vals)-1)\n",
    "        annot_name = \"{}-{}-labels.txt\".format(file_id, len(start_vals)-1)\n",
    "        ex, _ = extract_chunk(start_vals[-1], len(data), spec_name, annot_name, M_init=M_init)\n",
    "        examples.append(ex)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-80.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing scaling parameters based on TRAIN set only.\n",
    "min_bound, max_bound = get_minmax_bounds([p[0] for p in train_dataset])\n",
    "print(min_bound, max_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splits = [\n",
    "    (\"train\", train_dataset, NUM_TRAIN_SHARDS, TRAIN_CHUNK_SIZE_SEC, \"dense\", False),\n",
    "    (\"eval\", eval_dataset, NUM_EVAL_SHARDS, EVAL_CHUNK_SIZE_SEC, \"sparse\", True)\n",
    "]\n",
    "\n",
    "for splitname, dataset, num_shards, chunk_size, chunk_layout, drop_last_chunk in splits:\n",
    "    examples = []\n",
    "    for wav_filename, annot_filename in dataset:\n",
    "        examples.extend(process_file(wav_filename, annot_filename, min_bound, max_bound, chunk_size,\n",
    "                                     chunk_layout, drop_last_chunk, verbose=False))\n",
    "\n",
    "    with contextlib2.ExitStack() as tf_record_close_stack:\n",
    "        output_filebase = os.path.join(output_dir, \"{}_{}\".format(splitname, dataset_name))\n",
    "        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n",
    "            tf_record_close_stack, output_filebase, num_shards)\n",
    "        for index, example in enumerate(examples):\n",
    "            tf_example = create_tf_example(example)\n",
    "            output_shard_index = index % num_shards\n",
    "            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
