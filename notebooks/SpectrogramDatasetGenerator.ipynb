{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.io import wavfile\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_col, right_col = \"Begin Time (s)\", \"End Time (s)\"\n",
    "top_col, bot_col = \"High Freq (Hz)\", \"Low Freq (Hz)\"\n",
    "class_col, class_conf_col = \"Species\", \"Species Confidence\"\n",
    "\n",
    "recording_dir = \"../data\"\n",
    "annotation_dir = \"../data\"\n",
    "output_dir = \"../data/spectrograms-15s-072321\"\n",
    "\n",
    "# SPECTROGRAM CONSTANTS\n",
    "# Window size (n_fft) in seconds\n",
    "WINDOW_SIZE_SEC = 3/20\n",
    "# Hop Length in seconds\n",
    "HOP_LEN_SEC = 15/400\n",
    "# Number of frequency bands (y dimension of spectrogram)\n",
    "N_MELS = 400\n",
    "# Maximum frequency considered (highest value in y dimension)\n",
    "FREQUENCY_MAX = 1500\n",
    "\n",
    "# CHUNK CONSTANTS\n",
    "# Length of one chunk in seconds\n",
    "CHUNK_SIZE_SEC = 45\n",
    "# Minimum % visibility of a call to keep annotation\n",
    "MIN_BOX_PERCENT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_id(fname):\n",
    "    return path.basename(fname)[:22]\n",
    "\n",
    "# Constructs a list of pairs (wav_fname, annot_fname)\n",
    "def get_filename_pairs(recording_dir, annotation_dir):\n",
    "    wav_fnames = glob.glob(path.join(recording_dir, \"*.wav\"))\n",
    "    annot_fnames = glob.glob(path.join(annotation_dir, \"*.txt\"))\n",
    "    \n",
    "    res = []\n",
    "    id_set = set()\n",
    "    for wfname in wav_fnames:\n",
    "        id_nums = get_file_id(wfname)\n",
    "        if id_nums in id_set:\n",
    "            raise ValueError(\"Duplicate Wav ID: {}\".format(id_nums))\n",
    "        id_set.add(id_nums)\n",
    "        \n",
    "        annots = [a for a in annot_fnames if path.basename(a).startswith(id_nums)]\n",
    "        if len(annots) > 1:\n",
    "            raise ValueError(\"More than one annotation for recording: {}\".format(wfname))\n",
    "        if len(annots) < 1:\n",
    "            raise ValueError(\"No annotation for recording: {}\".format(wfname))\n",
    "        \n",
    "        res.append((wfname, annots[0]))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_filename_pairs(recording_dir, annotation_dir)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: measure speed of different fns for opening wav files\n",
    "def read_wavfile(wav_name, normalize=True, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Reading {}\".format(wav_name))\n",
    "    sr, data = wavfile.read(wav_name)\n",
    "    if verbose:\n",
    "        print(\"{} samples at {} samples/sec --> {} seconds\".format(data.shape[0], sr, data.shape[0]/sr))\n",
    "\n",
    "    if normalize:\n",
    "        data = data.astype(float)\n",
    "        data = data - data.min()\n",
    "        data = data / data.max()\n",
    "        data = data - 0.5\n",
    "    \n",
    "    return sr, data\n",
    "\n",
    "\n",
    "def read_annotations(fname, verbose=False):\n",
    "    annotations = pd.read_csv(fname, sep=\"\\t\")\n",
    "    if verbose:\n",
    "        print(\"Read {} annotations from {}\".format(len(annotations), fname))\n",
    "        print(\",\".join(annotations.columns))\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(annotation):\n",
    "    return ((annotation[right_col] - annotation[left_col])\n",
    "            * (annotation[top_col] - annotation[bot_col]))\n",
    "\n",
    "# Returns the min and max db observed in all files\n",
    "def get_db_bounds(wav_filenames):\n",
    "    min_val, max_val = None, None\n",
    "    for wfname in wav_filenames:\n",
    "        sr, data = read_wavfile(wfname, normalize=True)\n",
    "        n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "        hop_len = int(HOP_LEN_SEC * sr)\n",
    "        chunk_size = int(CHUNK_SIZE_SEC * sr)\n",
    "        for start_i in range(0, len(data), chunk_size - (hop_len * (N_MELS-2) + n_fft)):\n",
    "            mel_spec = librosa.feature.melspectrogram(y=data[start_i:min(len(data),start_i+chunk_size)],\n",
    "                                                      sr=sr,\n",
    "                                                      n_fft=n_fft,\n",
    "                                                      hop_length=hop_len,\n",
    "                                                      n_mels=N_MELS,\n",
    "                                                      fmax=FREQUENCY_MAX,\n",
    "                                                      center=False)\n",
    "            mel_spec = librosa.power_to_db(mel_spec)\n",
    "            temp_min = mel_spec.min()\n",
    "            temp_max = mel_spec.max()\n",
    "            if min_val is None or temp_min < min_val:\n",
    "                min_val = temp_min\n",
    "            if max_val is None or temp_max > max_val:\n",
    "                max_val = temp_max\n",
    "    return min_val, max_val\n",
    "\n",
    "def process_file(wav_filename, annot_filename, min_db, max_db, verbose=False):\n",
    "    sr, data = read_wavfile(wav_filename, normalize=True, verbose=verbose)\n",
    "    annotations = read_annotations(annot_filename, verbose=verbose)\n",
    "    file_id = get_file_id(wav_filename)\n",
    "    \n",
    "    n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "    hop_len = int(HOP_LEN_SEC * sr)\n",
    "    chunk_size = int(CHUNK_SIZE_SEC * sr)\n",
    "    \n",
    "    # Start Indices of each chunk\n",
    "    start_vals = [s for s in range(0, len(data), chunk_size - (hop_len * (N_MELS-2) + n_fft))]\n",
    "    \n",
    "    # If last cut point creates a tiny chunk, remove it\n",
    "    if len(data) - start_vals[-1] < int(chunk_size / 2):\n",
    "        start_vals = start_vals[:-1]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Start points: [{}]\".format(\",\".join([str(s) for s in start_vals])))\n",
    "        \n",
    "    def extract_chunk(start_i, end_i, spec_name, annot_name):\n",
    "        mel_spec = librosa.feature.melspectrogram(y=data[start_i:end_i],\n",
    "                                                  sr=sr,\n",
    "                                                  n_fft=n_fft,\n",
    "                                                  hop_length=hop_len,\n",
    "                                                  n_mels=N_MELS,\n",
    "                                                  fmax=FREQUENCY_MAX,\n",
    "                                                  center=False)\n",
    "        mel_spec = librosa.power_to_db(mel_spec)\n",
    "        mel_spec = np.clip((mel_spec - min_db) / (max_db - min_db) * 255, a_min=0, a_max=255)\n",
    "        mel_spec = mel_spec.astype(np.uint8)\n",
    "        spec_height, spec_width = mel_spec.shape\n",
    "        \n",
    "        # Get annotations to those inside chunk\n",
    "        start_s, end_s = start_i/sr, end_i/sr\n",
    "        freq_axis_low, freq_axis_high = librosa.hz_to_mel(0.0), librosa.hz_to_mel(FREQUENCY_MAX)\n",
    "        chunk_annotations = annotations.loc[~((annotations[left_col] > end_s)\n",
    "                                              | (annotations[right_col] < start_s))].copy()\n",
    "        # Rescale axes to 0.0-1.0 based on location inside chunk\n",
    "        chunk_annotations.loc[:,[left_col,right_col]] = ((chunk_annotations[[left_col,right_col]]\n",
    "                                                         - start_s) / (end_s - start_s)) * spec_width\n",
    "        chunk_annotations.loc[:,[bot_col,top_col]] = (1.0 - ((librosa.hz_to_mel(chunk_annotations[[bot_col,top_col]])\n",
    "                                                      - freq_axis_low) / (freq_axis_high - freq_axis_low))) * spec_height\n",
    "        trimmed_annots = chunk_annotations.copy()\n",
    "        trimmed_annots[left_col] = trimmed_annots[left_col].clip(lower=0, upper=spec_width).astype(int)\n",
    "        trimmed_annots[right_col] = trimmed_annots[right_col].clip(lower=0, upper=spec_width).astype(int)\n",
    "        trimmed_annots[bot_col] = trimmed_annots[bot_col].clip(lower=0, upper=spec_height).astype(int)\n",
    "        trimmed_annots[top_col] = trimmed_annots[top_col].clip(lower=0, upper=spec_height).astype(int)\n",
    "        overlaps = []\n",
    "        for i in trimmed_annots.index:\n",
    "            intersection = trimmed_annots.loc[i]\n",
    "            original = chunk_annotations.loc[i]\n",
    "            overlaps.append(get_area(intersection) / get_area(original))\n",
    "        chunk_annotations = trimmed_annots.loc[np.array(overlaps) > MIN_BOX_PERCENT]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Found {} annotations in chunk\".format(len(chunk_annotations)))\n",
    "        \n",
    "        # Save Chunk\n",
    "        im = Image.fromarray(mel_spec[::-1, :])\n",
    "        im = im.convert(\"L\")\n",
    "        im.save(path.join(output_dir, spec_name))\n",
    "        if verbose:\n",
    "            print(\"Saved spectrogram to '{}'\".format(spec_name))\n",
    "        chunk_annotations[\"image_name\"] = pd.Series({idx:spec_name for idx in chunk_annotations.index})\n",
    "        return chunk_annotations\n",
    "    \n",
    "    all_chunk_annotations = []\n",
    "    for ind, start_i in enumerate(start_vals[:-1]):\n",
    "        # Compute & Draw Mel Spectrogram\n",
    "        spec_name = \"{}-{}.png\".format(file_id, ind)\n",
    "        annot_name = \"{}-{}-labels.txt\".format(file_id, ind)\n",
    "        all_chunk_annotations.append(extract_chunk(start_i, start_i+chunk_size, spec_name, annot_name))\n",
    "    spec_name = \"{}-{}.png\".format(file_id, len(start_vals)-1)\n",
    "    annot_name = \"{}-{}-labels.txt\".format(file_id, len(start_vals)-1)\n",
    "    all_chunk_annotations.append(extract_chunk(start_vals[-1], len(data), spec_name, annot_name))\n",
    "    all_chunk_annotations = pd.concat(all_chunk_annotations)[\n",
    "        [\"image_name\", \"Species\", left_col, top_col, right_col, bot_col]\n",
    "    ]\n",
    "    classes = all_chunk_annotations[\"Species\"].unique()\n",
    "    class_map = {}\n",
    "    rev_class_map = {}\n",
    "    for i in range(len(classes)):\n",
    "        class_map[i+1] = classes[i]\n",
    "        rev_class_map[classes[i]] = i+1\n",
    "    pickle.dump(class_map, open(path.join(output_dir, \"classes.p\"), \"wb\"))\n",
    "    all_chunk_annotations[\"Species\"] = all_chunk_annotations[\"Species\"].map(rev_class_map)\n",
    "    all_chunk_annotations.to_csv(path.join(output_dir, \"all_labels.csv\"), index=False, header=False)\n",
    "    print(\"Saved annotations to '{}'\".format(\"all_labels.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_db, max_db = get_db_bounds([p[0] for p in dataset])\n",
    "print(min_db, max_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(output_dir)\n",
    "for wav_filename, annot_filename in dataset:\n",
    "    process_file(wav_filename, annot_filename, min_db, max_db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#  - Save metadata such as the mapping from y_px to hz and start,end time in seconds of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pickle.load(open(path.join(output_dir, \"classes.p\"), \"rb\"))\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(classes)+1)).tolist()\n",
    "labels = pd.read_csv(path.join(output_dir, \"all_labels.csv\"), header=None).to_numpy()\n",
    "img_name = labels[59,0]\n",
    "img_labels = labels[(labels[:,0] == img_name)]\n",
    "img_path = path.join(output_dir, img_name)\n",
    "with Image.open(img_path) as img:\n",
    "    img_data = np.array(img)\n",
    "    print(img_data.shape)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.imshow(img_data, cmap='gray')\n",
    "    \n",
    "    for box in img_labels:\n",
    "        xmin = box[2]\n",
    "        ymin = box[3]\n",
    "        xmax = box[4]\n",
    "        ymax = box[5]\n",
    "        label = '{}'.format(classes[int(box[1])])\n",
    "        color = colors[int(box[1])]\n",
    "        plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "        plt.gca().text(xmin+3, ymin-12, label, size='large', color='white', bbox={'facecolor':color, 'alpha':1.0})\n",
    "    \n",
    "    plt.title(img_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
